{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWyofTEXWAFi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_curve\n",
        "from sklearn.metrics import roc_auc_score, log_loss, confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "columns = [\n",
        "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
        "    \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n",
        "    \"hours-per-week\", \"native-country\", \"income\"\n",
        "]\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "df = pd.read_csv(url, header=None, names=columns, na_values=\" ?\", skipinitialspace=True)\n",
        "df.replace(\"?\", pd.NA, inplace=True)\n",
        "df = df.dropna()\n",
        "categorical_columns = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
        "df = pd.get_dummies(df, columns=categorical_columns)\n",
        "\n",
        "# print(df.head())\n",
        "\n",
        "column_to_move = df.columns[6]  # Assuming 0-based indexing\n",
        "df = df.assign(**{column_to_move: df.pop(column_to_move)})\n",
        "\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "columns_to_standardize = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "X_to_standardize = df[columns_to_standardize]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_standardized = pd.DataFrame(scaler.fit_transform(X_to_standardize), columns=columns_to_standardize)\n",
        "\n",
        "X = pd.concat([X_standardized, df.drop(columns=columns_to_standardize).reset_index(drop=True)], axis=1)\n",
        "\n",
        "X = X.iloc[:, :-1]\n",
        "\n",
        "y_new = []\n",
        "for val in y.values:\n",
        "  if val[0] == \"<\":\n",
        "    y_new.append(0)\n",
        "  else:\n",
        "    y_new.append(1)\n",
        "\n",
        "y = np.array(y_new)\n",
        "X = X.values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "max_components = min(X_test.shape[0], X_test.shape[1])\n",
        "print(\"Maximum number of components:\", max_components)\n",
        "\n",
        "num_components= [5,50,75]\n",
        "for every in num_components:\n",
        "    # Apply PCA for dimensionality reduction (optional)\n",
        "    # You can skip this step or adjust the number of components based on your needs\n",
        "    # num_components = 15  # You can choose the number of components you want\n",
        "    pca = PCA(n_components=every)\n",
        "    # Fit and transform the training data\n",
        "    X_train_transformed = pca.fit_transform(X_train)\n",
        "    # Transform the test data\n",
        "    X_test_transformed = pca.transform(X_test)\n",
        "    # Visualize the first two principal components\n",
        "    explained_var_ratio = pca.explained_variance_ratio_\n",
        "    # Scree plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(1, every + 1), explained_var_ratio, alpha=0.7, align='center')\n",
        "    plt.title('Explained Variance Ratio for Each Principal Component')\n",
        "    plt.xlabel('Principal Component')\n",
        "    plt.ylabel('Explained Variance Ratio')\n",
        "    plt.show()\n",
        "\n",
        "    # Define the model\n",
        "    model = KNeighborsClassifier()\n",
        "\n",
        "    # Define the hyperparameter grid\n",
        "    param_grid = {\n",
        "        'n_neighbors': [3,5,7,9],\n",
        "        'weights': ['uniform','distance'],\n",
        "        'p': [1,2]\n",
        "    }\n",
        "\n",
        "    # Create GridSearchCV object with cross-validation\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train_transformed, y_train)\n",
        "\n",
        "    # Get the best parameters and the corresponding model\n",
        "    best_params = grid_search.best_params_\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    all_models = grid_search.cv_results_['params']\n",
        "\n",
        "    # Use cross_val_score to get cross-validation scores for each model\n",
        "    cross_val_scores = []\n",
        "    for model_params in all_models:\n",
        "        model = KNeighborsClassifier(**model_params)\n",
        "        scores = cross_val_score(model, X_train_transformed, y_train, cv=5, scoring='accuracy')\n",
        "        cross_val_scores.append(scores)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.boxplot(data=cross_val_scores)\n",
        "    plt.xticks(ticks=range(len(all_models)))\n",
        "\n",
        "    # Simulate partial_fit with batch-wise training\n",
        "    batch_size = X_train.shape[0]\n",
        "    num_samples = X_train.shape[0]\n",
        "    num_batches = int(np.ceil(num_samples / batch_size))\n",
        "    print(\"Best model : \", best_params)\n",
        "\n",
        "    # best_sgd_model = KNeighborsClassifier(**best_params)\n",
        "    best_knn_model = KNeighborsClassifier(n_neighbors=best_params['n_neighbors'])\n",
        "    # Train the model\n",
        "    best_knn_model.fit(X_train_transformed, y_train)\n",
        "\n",
        "     # Evaluate the best model on the test set\n",
        "    y_pred = best_knn_model.predict(X_test_transformed)\n",
        "    y_prob = best_knn_model.predict_proba(X_test_transformed)\n",
        "\n",
        "    # Testing accuracy\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Testing Accuracy:\", test_accuracy)\n",
        "\n",
        "    # Classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Confusion matrix\n",
        "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_mat)\n"
      ]
    }
  ]
}