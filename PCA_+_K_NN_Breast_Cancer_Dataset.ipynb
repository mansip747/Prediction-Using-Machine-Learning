{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWyofTEXWAFi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_curve\n",
        "from sklearn.metrics import roc_auc_score, log_loss, confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Fetch dataset\n",
        "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)\n",
        "\n",
        "# Data (as pandas dataframes)\n",
        "X = breast_cancer_wisconsin_diagnostic.data.features\n",
        "y = breast_cancer_wisconsin_diagnostic.data.targets\n",
        "\n",
        "# Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "class_mapping = {'M': 1, 'B': 0}\n",
        "y = [class_mapping[target] for target in y.T.values[0]]\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "max_components = min(X_test.shape[0], X_test.shape[1])\n",
        "print(\"Maximum number of components:\", max_components)\n",
        "\n",
        "num_components= [10, 15, 20]\n",
        "for every in num_components:\n",
        "    # Apply PCA for dimensionality reduction (optional)\n",
        "    # You can skip this step or adjust the number of components based on your needs\n",
        "    # num_components = 15  # You can choose the number of components you want\n",
        "    pca = PCA(n_components=every)\n",
        "    # Fit and transform the training data\n",
        "    X_train_transformed = pca.fit_transform(X_train)\n",
        "    # Transform the test data\n",
        "    X_test_transformed = pca.transform(X_test)\n",
        "    # Visualize the first two principal components\n",
        "    explained_var_ratio = pca.explained_variance_ratio_\n",
        "    # Scree plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(1, every + 1), explained_var_ratio, alpha=0.7, align='center')\n",
        "    plt.title('Explained Variance Ratio for Each Principal Component')\n",
        "    plt.xlabel('Principal Component')\n",
        "    plt.ylabel('Explained Variance Ratio')\n",
        "    plt.show()\n",
        "\n",
        "    # Define the model\n",
        "    model = KNeighborsClassifier()\n",
        "\n",
        "    # Define the hyperparameter grid\n",
        "    param_grid = {\n",
        "        'n_neighbors': [2,3,4,5,6,7,8],\n",
        "        'weights': ['uniform','distance'],\n",
        "        'p': [1,2]\n",
        "    }\n",
        "\n",
        "    # Create GridSearchCV object with cross-validation\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train_transformed, y_train)\n",
        "\n",
        "    # Get the best parameters and the corresponding model\n",
        "    best_params = grid_search.best_params_\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    all_models = grid_search.cv_results_['params']\n",
        "\n",
        "    # Use cross_val_score to get cross-validation scores for each model\n",
        "    cross_val_scores = []\n",
        "    for model_params in all_models:\n",
        "        model = KNeighborsClassifier(**model_params)\n",
        "        scores = cross_val_score(model, X_train_transformed, y_train, cv=5, scoring='accuracy')\n",
        "        cross_val_scores.append(scores)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.boxplot(data=cross_val_scores)\n",
        "    plt.xticks(ticks=range(len(all_models)))\n",
        "\n",
        "    # Simulate partial_fit with batch-wise training\n",
        "    batch_size = X_train.shape[0]\n",
        "    num_samples = X_train.shape[0]\n",
        "    num_batches = int(np.ceil(num_samples / batch_size))\n",
        "    print(\"Best model : \", best_params)\n",
        "\n",
        "    # best_sgd_model = SGDClassifier(**best_params)\n",
        "    best_knn_model = KNeighborsClassifier(n_neighbors=best_params['n_neighbors'])\n",
        "\n",
        "        # Train the model\n",
        "    best_knn_model.fit(X_train_transformed, y_train)\n",
        "\n",
        "         # Evaluate the best model on the test set\n",
        "    y_pred = best_knn_model.predict(X_test_transformed)\n",
        "    y_prob = best_knn_model.predict_proba(X_test_transformed)\n",
        "\n",
        "    # Testing accuracy\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Testing Accuracy:\", test_accuracy)\n",
        "\n",
        "    # Classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Confusion matrix\n",
        "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_mat)\n"
      ]
    }
  ]
}