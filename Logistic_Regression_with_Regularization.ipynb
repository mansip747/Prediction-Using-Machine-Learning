{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression With Regularization\n",
        "\n",
        "# UCI Adult Dataset"
      ],
      "metadata": {
        "id": "aqqKvuoFYR7x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWyofTEXWAFi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_curve\n",
        "from sklearn.metrics import roc_auc_score, log_loss, confusion_matrix\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "columns = [\n",
        "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
        "    \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n",
        "    \"hours-per-week\", \"native-country\", \"income\"\n",
        "]\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "df = pd.read_csv(url, header=None, names=columns, na_values=\" ?\", skipinitialspace=True)\n",
        "df.replace(\"?\", pd.NA, inplace=True)\n",
        "df = df.dropna()\n",
        "categorical_columns = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
        "df = pd.get_dummies(df, columns=categorical_columns)\n",
        "\n",
        "# print(df.head())\n",
        "\n",
        "column_to_move = df.columns[6]  # Assuming 0-based indexing\n",
        "df = df.assign(**{column_to_move: df.pop(column_to_move)})\n",
        "\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "columns_to_standardize = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "X_to_standardize = df[columns_to_standardize]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_standardized = pd.DataFrame(scaler.fit_transform(X_to_standardize), columns=columns_to_standardize)\n",
        "\n",
        "X = pd.concat([X_standardized, df.drop(columns=columns_to_standardize).reset_index(drop=True)], axis=1)\n",
        "\n",
        "X = X.iloc[:, :-1]\n",
        "\n",
        "y_new = []\n",
        "for val in y.values:\n",
        "  if val[0] == \"<\":\n",
        "    y_new.append(0)\n",
        "  else:\n",
        "    y_new.append(1)\n",
        "\n",
        "y = np.array(y_new)\n",
        "X = X.values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = SGDClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'alpha': [0.00001, 0.0001, 0.001, 0.01, .1],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'max_iter': [1000],\n",
        "    'loss' : ['log_loss'],\n",
        "    'learning_rate' : ['constant'],\n",
        "    'eta0' : [0.00001, 0.0001, 0.001, 0.01, .1]\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object with cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the corresponding model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "all_models = grid_search.cv_results_['params']\n",
        "\n",
        "# Use cross_val_score to get cross-validation scores for each model\n",
        "cross_val_scores = []\n",
        "for model_params in all_models:\n",
        "    model = SGDClassifier(**model_params, random_state=42)\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    cross_val_scores.append(scores)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(data=cross_val_scores)\n",
        "plt.xticks(ticks=range(len(all_models)))\n",
        "\n",
        "\n",
        "# Simulate partial_fit with batch-wise training\n",
        "batch_size = X_train.shape[0]\n",
        "num_samples = X_train.shape[0]\n",
        "num_batches = int(np.ceil(num_samples / batch_size))\n",
        "print(\"Best model : \", best_params)\n",
        "\n",
        "# Training and validation losses\n",
        "epochs = [20, 80]\n",
        "for each in epochs:\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    # best_sgd_model = SGDClassifier(**best_params)\n",
        "    best_sgd_model = SGDClassifier(alpha=best_params['alpha'], eta0=best_params['eta0'],\n",
        "                                   learning_rate=best_params['learning_rate'],loss=best_params['loss'],\n",
        "                                   max_iter=best_params['max_iter'],penalty=best_params['penalty'],random_state=42)\n",
        "\n",
        "    # Learning rate schedule\n",
        "    initial_learning_rate = 0.01\n",
        "    learning_rate = initial_learning_rate\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    best_val_loss = float('inf')\n",
        "    no_improvement_count = 0\n",
        "\n",
        "    for epoch in range(each):\n",
        "        for batch_start in range(0, num_samples, batch_size):\n",
        "            batch_end = min(batch_start + batch_size, num_samples)\n",
        "            X_batch = X_train[batch_start:batch_end]\n",
        "            y_batch = y_train[batch_start:batch_end]\n",
        "\n",
        "            # Train the model on the batch\n",
        "            best_sgd_model.partial_fit(X_batch, y_batch, classes=np.unique(y_train))\n",
        "        # print(epoch)\n",
        "\n",
        "        # Evaluate on training and validation sets\n",
        "        train_predictions = best_sgd_model.predict(X_train)\n",
        "        val_predictions = best_sgd_model.predict(X_test)\n",
        "        train_loss = log_loss(y_train, train_predictions)\n",
        "        val_loss = log_loss(y_test, val_predictions)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "        val_accuracy = accuracy_score(y_test, val_predictions)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Log Loss')\n",
        "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Log Loss')\n",
        "    plt.title('Training and Validation Log Loss Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Log Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Training Accuracy')\n",
        "    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Evaluate the best model on the test set\n",
        "    y_pred = best_sgd_model.predict(X_test)\n",
        "    y_prob = best_sgd_model.decision_function(X_test)\n",
        "\n",
        "    # Testing accuracy\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Testing Accuracy:\", test_accuracy)\n",
        "\n",
        "    # Classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Confusion matrix\n",
        "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_mat)\n",
        "\n",
        "\n",
        "    # ROC curve and AUC score\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "    auc_score = roc_auc_score(y_test, y_prob)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Breast Cancer Dataset"
      ],
      "metadata": {
        "id": "eVTpS8z3YnKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_curve\n",
        "from sklearn.metrics import roc_auc_score, log_loss, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Fetch dataset\n",
        "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)\n",
        "\n",
        "# Data (as pandas dataframes)\n",
        "X = breast_cancer_wisconsin_diagnostic.data.features\n",
        "y = breast_cancer_wisconsin_diagnostic.data.targets\n",
        "\n",
        "# Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "class_mapping = {'M': 1, 'B': 0}\n",
        "y = [class_mapping[target] for target in y.T.values[0]]\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = SGDClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'max_iter': [1000],\n",
        "    'loss' : ['log_loss'],\n",
        "    'learning_rate' : ['constant'],\n",
        "    'eta0' : [0.0001, 0.001, 0.01, 0.1]\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object with cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the corresponding model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "all_models = grid_search.cv_results_['params']\n",
        "\n",
        "# Use cross_val_score to get cross-validation scores for each model\n",
        "cross_val_scores = []\n",
        "for model_params in all_models:\n",
        "    model = SGDClassifier(**model_params, random_state=42)\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    cross_val_scores.append(scores)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(data=cross_val_scores)\n",
        "plt.xticks(ticks=range(len(all_models)))\n",
        "\n",
        "\n",
        "# Simulate partial_fit with batch-wise training\n",
        "batch_size = X_train.shape[0]\n",
        "num_samples = X_train.shape[0]\n",
        "num_batches = int(np.ceil(num_samples / batch_size))\n",
        "print(\"Best model : \", best_params)\n",
        "\n",
        "# Training and validation losses\n",
        "epochs = [5, 8, 10]\n",
        "for each in epochs:\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    # best_sgd_model = SGDClassifier(**best_params)\n",
        "    best_sgd_model = SGDClassifier(alpha=best_params['alpha'], eta0=best_params['eta0'],\n",
        "                                   learning_rate=best_params['learning_rate'],loss=best_params['loss'],\n",
        "                                   max_iter=best_params['max_iter'],penalty=best_params['penalty'],random_state=42)\n",
        "\n",
        "    # Learning rate schedule\n",
        "    initial_learning_rate = 0.01\n",
        "    learning_rate = initial_learning_rate\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    best_val_loss = float('inf')\n",
        "    no_improvement_count = 0\n",
        "\n",
        "    for epoch in range(each):\n",
        "        for batch_start in range(0, num_samples, batch_size):\n",
        "            batch_end = min(batch_start + batch_size, num_samples)\n",
        "            X_batch = X_train[batch_start:batch_end]\n",
        "            y_batch = y_train[batch_start:batch_end]\n",
        "\n",
        "            # Train the model on the batch\n",
        "            best_sgd_model.partial_fit(X_batch, y_batch, classes=np.unique(y_train))\n",
        "\n",
        "        # Evaluate on training and validation sets\n",
        "        train_predictions = best_sgd_model.predict(X_train)\n",
        "        val_predictions = best_sgd_model.predict(X_test)\n",
        "        train_loss = log_loss(y_train, train_predictions)\n",
        "        val_loss = log_loss(y_test, val_predictions)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "        val_accuracy = accuracy_score(y_test, val_predictions)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        # Learning rate schedule (decay every 10 epochs)\n",
        "        if epoch % 10 == 0:\n",
        "            learning_rate = learning_rate * 0.9\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Log Loss')\n",
        "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Log Loss')\n",
        "    plt.title('Training and Validation Log Loss Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Log Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Training Accuracy')\n",
        "    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Evaluate the best model on the test set\n",
        "    y_pred = best_sgd_model.predict(X_test)\n",
        "    y_prob = best_sgd_model.decision_function(X_test)\n",
        "\n",
        "    # Testing accuracy\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Testing Accuracy:\", test_accuracy)\n",
        "\n",
        "    # Classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Confusion matrix\n",
        "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_mat)\n",
        "\n",
        "\n",
        "    # ROC curve and AUC score\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "    auc_score = roc_auc_score(y_test, y_prob)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "aftrqAUbYvhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fashion MNIST Dataset"
      ],
      "metadata": {
        "id": "90y-_jQQYxAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_curve\n",
        "from sklearn.metrics import roc_auc_score, log_loss, confusion_matrix\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load and preprocess the Fashion MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_train = X_train.reshape((60000, 28, 28)).astype('float32')\n",
        "X_test = X_test.reshape((10000, 28, 28)).astype('float32')\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "# fashion_mnist = datasets.fetch_openml('Fashion-MNIST', version=1, cache=True)\n",
        "X_train = X_train.reshape(60000, 28*28)\n",
        "X_test = X_test.reshape(10000, 28*28)\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "X_test_std = scaler.transform(X_test)\n",
        "\n",
        "# Apply PCA for dimensionality reduction (optional)\n",
        "# You can skip this step or adjust the number of components based on your needs\n",
        "num_components = 50  # You can choose the number of components you want\n",
        "pca = PCA(n_components=num_components)\n",
        "# Fit and transform the training data\n",
        "X_train = pca.fit_transform(X_train_std)\n",
        "# Transform the test data\n",
        "X_test = pca.transform(X_test_std)\n",
        "# Visualize the first two principal components\n",
        "explained_var_ratio = pca.explained_variance_ratio_\n",
        "# Scree plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(1, num_components + 1), explained_var_ratio, alpha=0.7, align='center')\n",
        "plt.title('Explained Variance Ratio for Each Principal Component')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.show()\n",
        "\n",
        "# Define the model\n",
        "model = SGDClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'alpha': [0.00001, 0.0001, 0.001, 0.01],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'max_iter': [1000],\n",
        "    'loss' : ['log_loss'],\n",
        "    'learning_rate' : ['constant'],\n",
        "    'eta0' : [0.0001, 0.001, 0.01, 0.1]\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object with cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the corresponding model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "all_models = grid_search.cv_results_['params']\n",
        "\n",
        "# Use cross_val_score to get cross-validation scores for each model\n",
        "cross_val_scores = []\n",
        "for model_params in all_models:\n",
        "    model = SGDClassifier(**model_params, random_state=42)\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    cross_val_scores.append(scores)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(data=cross_val_scores)\n",
        "plt.xticks(ticks=range(len(all_models)))\n",
        "\n",
        "\n",
        "# Simulate partial_fit with batch-wise training\n",
        "batch_size = X_train.shape[0]\n",
        "num_samples = X_train.shape[0]\n",
        "num_batches = int(np.ceil(num_samples / batch_size))\n",
        "print(\"Best model : \", best_params)\n",
        "\n",
        "# Training and validation losses\n",
        "epochs = [60, 80]\n",
        "for each in epochs:\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    # best_sgd_model = SGDClassifier(**best_params)\n",
        "    best_sgd_model = SGDClassifier(alpha=best_params['alpha'], eta0=best_params['eta0'],\n",
        "                                   learning_rate=best_params['learning_rate'],loss=best_params['loss'],\n",
        "                                   max_iter=best_params['max_iter'],penalty=best_params['penalty'],random_state=42)\n",
        "\n",
        "    # Learning rate schedule\n",
        "    initial_learning_rate = 0.01\n",
        "    learning_rate = initial_learning_rate\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    best_val_loss = float('inf')\n",
        "    no_improvement_count = 0\n",
        "\n",
        "    for epoch in range(each):\n",
        "        for batch_start in range(0, num_samples, batch_size):\n",
        "            batch_end = min(batch_start + batch_size, num_samples)\n",
        "            X_batch = X_train[batch_start:batch_end]\n",
        "            y_batch = y_train[batch_start:batch_end]\n",
        "\n",
        "            # Train the model on the batch\n",
        "            best_sgd_model.partial_fit(X_batch, y_batch, classes=np.unique(y_train))\n",
        "        # print(epoch)\n",
        "\n",
        "        # Evaluate on training and validation sets\n",
        "        train_predictions = best_sgd_model.predict(X_train)\n",
        "        val_predictions = best_sgd_model.predict(X_test)\n",
        "        train_probability = best_sgd_model.predict_proba(X_train)\n",
        "        val_probability = best_sgd_model.predict_proba(X_test)\n",
        "        # print(y_train)\n",
        "        # print(train_predictions)\n",
        "        train_loss = log_loss(y_train, train_probability)\n",
        "        val_loss = log_loss(y_test, val_probability)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "        val_accuracy = accuracy_score(y_test, val_predictions)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        # # Learning rate schedule (decay every 10 epochs)\n",
        "        # if epoch % 10 == 0:\n",
        "        #     learning_rate = learning_rate * 0.9\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Log Loss')\n",
        "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Log Loss')\n",
        "    plt.title('Training and Validation Log Loss Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Log Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Training Accuracy')\n",
        "    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Evaluate the best model on the test set\n",
        "    y_pred = best_sgd_model.predict(X_test)\n",
        "    y_prob = best_sgd_model.decision_function(X_test)\n",
        "\n",
        "    # Testing accuracy\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Testing Accuracy:\", test_accuracy)\n",
        "\n",
        "    # Classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Confusion matrix\n",
        "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_mat)"
      ],
      "metadata": {
        "id": "vnjIs0buYxfl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}